{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "batch_size = 64\n",
    "mean = torch.load('.\\statistics\\mnist_mean.pt')\n",
    "(full_dim, mid_dim, hidden) = (1 * 28 * 28, 1000, 5)\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "trainset = torchvision.datasets.MNIST(root='.\\data\\MNIST',train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Additive coupling layer.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Log-scaling layer.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'NICE main model.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dequantize(x):\n",
    "    '''Dequantize data.\n",
    "\n",
    "    Add noise sampled from Uniform(0, 1) to each pixel (in [0, 255]).\n",
    "\n",
    "    Args:\n",
    "        x: input tensor.\n",
    "        reverse: True in inference mode, False in training mode.\n",
    "    Returns:\n",
    "        dequantized data.\n",
    "    '''\n",
    "    noise = torch.distributions.Uniform(0., 1.).sample(x.size())\n",
    "    return (x * 255. + noise) / 256.\n",
    "\n",
    "def prepare_data(x, mean=None, reverse=False):\n",
    "    \"\"\"Prepares data for NICE.\n",
    "\n",
    "    In training mode, flatten and dequantize the input.\n",
    "    In inference mode, reshape tensor into image size.\n",
    "\n",
    "    Args:\n",
    "        x: input minibatch.\n",
    "        zca: ZCA whitening transformation matrix.\n",
    "        mean: center of original dataset.\n",
    "        reverse: True if in inference mode, False if in training mode.\n",
    "    Returns:\n",
    "        transformed data.\n",
    "    \"\"\"\n",
    "    if reverse:\n",
    "        assert len(list(x.size())) == 2\n",
    "        [B, W] = list(x.size())\n",
    "        assert W == 1 * 28 * 28\n",
    "        x += mean\n",
    "        x = x.reshape((B, 1, 28, 28))\n",
    "    else:\n",
    "        assert len(list(x.size())) == 4\n",
    "        [B, C, H, W] = list(x.size())\n",
    "        assert [C, H, W] == [1, 28, 28]\n",
    "        x = dequantize(x)\n",
    "        x = x.reshape((B, C*H*W))\n",
    "        x -= mean\n",
    "    return x\n",
    "#%%\n",
    "# import matplotlib.pyplot as plt\n",
    "# n, bins, patches = plt.hist(x[:10].flatten().numpy(), 50, density=True, facecolor='g', alpha=0.75)\n",
    "# plt.show()\n",
    "# y = dequantize(x)\n",
    "# y = y.reshape(-1,784)-mean\n",
    "# n, bins, patches = plt.hist(y[:10].flatten().numpy(), 50, density=True, facecolor='g', alpha=0.75)\n",
    "# plt.show()\n",
    "#%%\n",
    "class StandardLogistic(torch.distributions.Distribution):\n",
    "    def __init__(self):\n",
    "        super(StandardLogistic, self).__init__()\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        \"\"\"Computes data log-likelihood.\n",
    "\n",
    "        Args:\n",
    "            x: input tensor.\n",
    "        Returns:\n",
    "            log-likelihood.\n",
    "        \"\"\"\n",
    "        return -(F.softplus(x) + F.softplus(-x))\n",
    "\n",
    "    def sample(self, size):\n",
    "        \"\"\"Samples from the distribution.\n",
    "\n",
    "        Args:\n",
    "            size: number of samples to generate.\n",
    "        Returns:\n",
    "            samples.\n",
    "        \"\"\"\n",
    "        z = torch.distributions.Uniform(0., 1.).sample(size)\n",
    "        return torch.log(z) - torch.log(1. - z)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"Additive coupling layer.\n",
    "\"\"\"\n",
    "class Coupling(nn.Module):\n",
    "    def __init__(self, in_out_dim, mid_dim, hidden, mask_config):\n",
    "        \"\"\"Initialize a coupling layer.\n",
    "\n",
    "        Args:\n",
    "            in_out_dim: input/output dimensions.\n",
    "            mid_dim: number of units in a hidden layer.\n",
    "            hidden: number of hidden layers.\n",
    "            mask_config: 1 if transform odd units, 0 if transform even units.\n",
    "        \"\"\"\n",
    "        super(Coupling, self).__init__()\n",
    "        self.mask_config = mask_config\n",
    "        self.in_block = nn.Sequential(nn.Linear(in_out_dim//2, mid_dim),nn.ReLU())\n",
    "        self.mid_block = nn.ModuleList([nn.Sequential(nn.Linear(mid_dim, mid_dim),nn.ReLU())for _ in range(hidden - 1)])\n",
    "        self.out_block = nn.Linear(mid_dim, in_out_dim//2)\n",
    "    def forward(self, x, reverse=False):\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: input tensor.\n",
    "            reverse: True in inference mode, False in sampling mode.\n",
    "        Returns:\n",
    "            transformed tensor.\n",
    "        \"\"\"\n",
    "        [B, W] = list(x.size())\n",
    "        x = x.reshape((B, W//2, 2))\n",
    "        if self.mask_config:\n",
    "            on, off = x[:, :, 0], x[:, :, 1]\n",
    "        else:\n",
    "            off, on = x[:, :, 0], x[:, :, 1]\n",
    "\n",
    "        off_ = self.in_block(off)\n",
    "        for i in range(len(self.mid_block)):\n",
    "            off_ = self.mid_block[i](off_)\n",
    "        shift = self.out_block(off_)\n",
    "\n",
    "        if reverse:\n",
    "            on = on - shift\n",
    "        else:\n",
    "            on = on + shift\n",
    "\n",
    "        if self.mask_config:\n",
    "            x = torch.stack((on, off), dim=2)\n",
    "        else:\n",
    "            x = torch.stack((off, on), dim=2)\n",
    "        return x.reshape((B, W))\n",
    "\n",
    "\"\"\"Log-scaling layer.\n",
    "\"\"\"\n",
    "class Scaling(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        \"\"\"Initialize a (log-)scaling layer.\n",
    "\n",
    "        Args:\n",
    "            dim: input/output dimensions.\n",
    "        \"\"\"\n",
    "        super(Scaling, self).__init__()\n",
    "        self.scale = nn.Parameter(torch.zeros((1, dim)),\n",
    "                                  requires_grad=True)\n",
    "\n",
    "    def forward(self, x, reverse=False):\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: input tensor.\n",
    "            reverse: True in inference mode, False in sampling mode.\n",
    "        Returns:\n",
    "            transformed tensor and log-determinant of Jacobian.\n",
    "        \"\"\"\n",
    "        log_det_J = torch.sum(self.scale)\n",
    "        if reverse:\n",
    "            x = x * torch.exp(-self.scale)\n",
    "        else:\n",
    "            x = x * torch.exp(self.scale)\n",
    "        return x, log_det_J\n",
    "\n",
    "\"\"\"NICE main model.\n",
    "\"\"\"\n",
    "class NICE(nn.Module):\n",
    "    def __init__(self, prior, coupling,in_out_dim, mid_dim, hidden, mask_config):\n",
    "        \"\"\"Initialize a NICE.\n",
    "\n",
    "        Args:\n",
    "            prior: prior distribution over latent space Z.\n",
    "            coupling: number of coupling layers.\n",
    "            in_out_dim: input/output dimensions.\n",
    "            mid_dim: number of units in a hidden layer.\n",
    "            hidden: number of hidden layers.\n",
    "            mask_config: 1 if transform odd units, 0 if transform even units.\n",
    "        \"\"\"\n",
    "        super(NICE, self).__init__()\n",
    "        self.prior = prior\n",
    "        self.in_out_dim = in_out_dim\n",
    "        self.coupling = nn.ModuleList([\n",
    "            Coupling(in_out_dim=in_out_dim,mid_dim=mid_dim,hidden=hidden,\n",
    "                     mask_config=(mask_config+i)%2) \\\n",
    "                     for i in range(coupling)])\n",
    "        self.scaling = Scaling(in_out_dim)\n",
    "\n",
    "    def g(self, z):\n",
    "        \"\"\"Transformation g: Z -> X (inverse of f).\n",
    "\n",
    "        Args:\n",
    "            z: tensor in latent space Z.\n",
    "        Returns:\n",
    "            transformed tensor in data space X.\n",
    "        \"\"\"\n",
    "        x, _ = self.scaling(z, reverse=True)\n",
    "        for i in reversed(range(len(self.coupling))):\n",
    "            x = self.coupling[i](x, reverse=True)\n",
    "        return x\n",
    "\n",
    "    def f(self, x):\n",
    "        \"\"\"Transformation f: X -> Z (inverse of g).\n",
    "\n",
    "        Args:\n",
    "            x: tensor in data space X.\n",
    "        Returns:\n",
    "            transformed tensor in latent space Z.\n",
    "        \"\"\"\n",
    "        for i in range(len(self.coupling)):\n",
    "            x = self.coupling[i](x)\n",
    "        return self.scaling(x)\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        \"\"\"Computes data log-likelihood.\n",
    "\n",
    "        (See Section 3.3 in the NICE paper.)\n",
    "\n",
    "        Args:\n",
    "            x: input minibatch.\n",
    "        Returns:\n",
    "            log-likelihood of input.\n",
    "        \"\"\"\n",
    "        z, log_det_J = self.f(x)\n",
    "        log_ll = torch.sum(self.prior.log_prob(z), dim=1)\n",
    "        return log_ll + log_det_J\n",
    "\n",
    "    def sample(self, size):\n",
    "        \"\"\"Generates samples.\n",
    "\n",
    "        Args:\n",
    "            size: number of samples to generate.\n",
    "        Returns:\n",
    "            samples from the data space X.\n",
    "        \"\"\"\n",
    "        z = self.prior.sample((size, self.in_out_dim))\n",
    "        return self.g(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: input minibatch.\n",
    "        Returns:\n",
    "            log-likelihood of input.\n",
    "        \"\"\"\n",
    "        return self.log_prob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = StandardLogistic()\n",
    "coupling = 4\n",
    "mask_config = 1\n",
    "flow = NICE(prior=prior,\n",
    "            coupling=coupling,\n",
    "            in_out_dim=full_dim,\n",
    "            mid_dim=mid_dim,\n",
    "            hidden=hidden,\n",
    "            mask_config=mask_config).to(device)\n",
    "optimizer = torch.optim.Adam(flow.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:03,  5.42it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-8e18ae04c3fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# clear gradient tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0msteps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\dlpt\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\dlpt\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'betas'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m             F.adam(params_with_grad,\n\u001b[0m\u001b[0;32m    109\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\dlpt\\lib\\site-packages\\torch\\optim\\functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train = True\n",
    "running_loss = 0\n",
    "max_iter = 50\n",
    "mean_loss_every_100_steps = []\n",
    "for iter in range(max_iter):\n",
    "    steps = 0\n",
    "    for _, data in tqdm(enumerate(trainloader, 1)):\n",
    "        inputs, _ = data\n",
    "        inputs = prepare_data(inputs, mean=mean).to(device)\n",
    "\n",
    "        # log-likelihood of input minibatch\n",
    "        loss = -flow(inputs).mean()\n",
    "        running_loss += float(loss)\n",
    "\n",
    "        # backprop and update parameters\n",
    "        optimizer.zero_grad()  # clear gradient tensor\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        steps += 1\n",
    "        if steps % 100 == 0:\n",
    "            mean_loss = running_loss / 100\n",
    "            print('iter %s:' % iter,'loss = %.3f' % mean_loss)\n",
    "            mean_loss_every_100_steps.append(mean_loss)\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "                z, _ = flow.f(inputs)\n",
    "                reconst = flow.g(z).cpu()\n",
    "                reconst = prepare_data(reconst, mean=mean, reverse=True)\n",
    "                samples = flow.sample(20).cpu()\n",
    "                samples = prepare_data(samples, mean=mean, reverse=True)\n",
    "                torchvision.utils.save_image(torchvision.utils.make_grid(reconst),'./reconstruction/'  +'iter%d_step%d.png' % (iter,steps),nrow=4)\n",
    "                torchvision.utils.save_image(torchvision.utils.make_grid(samples),'./samples/'  +'iter%d_step%d.png' % (iter,steps),nrow=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
